# Assignment-for-Research-and-Development-AI
Abstract

This study presents a systematic approach to estimate the unknown parameters Î¸, M, and X of a parametric curve using constrained optimization through the L-BFGS-B algorithm.
By uniformly sampling parameter values and minimizing the L1 distance between predicted and observed coordinates, the model achieved stable convergence and high accuracy.
The final optimized parameters:

ğœƒ
=
0.4908
 rad 
(
28.12
Â°
)
,
ğ‘€
=
0.02138
,
ğ‘‹
=
54.90195
Î¸=0.4908 rad (28.12Â°),M=0.02138,X=54.90195

resulted in an L1 distance of 25.24 across 1500 points, confirming an accurate and reliable curve fit.

FINAL DESMOS SUBMISSION
(
ğ‘¡
âˆ—
cos
â¡
(
0.4908
)
âˆ’
ğ‘’
0.02138
âˆ£
ğ‘¡
âˆ£
sin
â¡
(
0.3
ğ‘¡
)
sin
â¡
(
0.4908
)
+
54.90195
,
 
42
+
ğ‘¡
âˆ—
sin
â¡
(
0.4908
)
+
ğ‘’
0.02138
âˆ£
ğ‘¡
âˆ£
sin
â¡
(
0.3
ğ‘¡
)
cos
â¡
(
0.4908
)
)
(tâˆ—cos(0.4908)âˆ’e
0.02138âˆ£tâˆ£
sin(0.3t)sin(0.4908)+54.90195, 42+tâˆ—sin(0.4908)+e
0.02138âˆ£tâˆ£
sin(0.3t)cos(0.4908))
1) Introduction
1.1 Problem Formulation

The goal of this study is to estimate the optimal parameters for the nonlinear parametric curve:

{
ğ‘¥
(
ğ‘¡
)
=
ğ‘¡
cos
â¡
(
ğœƒ
)
âˆ’
ğ‘’
ğ‘€
âˆ£
ğ‘¡
âˆ£
sin
â¡
(
0.3
ğ‘¡
)
sin
â¡
(
ğœƒ
)
+
ğ‘‹


ğ‘¦
(
ğ‘¡
)
=
42
+
ğ‘¡
sin
â¡
(
ğœƒ
)
+
ğ‘’
ğ‘€
âˆ£
ğ‘¡
âˆ£
sin
â¡
(
0.3
ğ‘¡
)
cos
â¡
(
ğœƒ
)
{
x(t)=tcos(Î¸)âˆ’e
Mâˆ£tâˆ£
sin(0.3t)sin(Î¸)+X
y(t)=42+tsin(Î¸)+e
Mâˆ£tâˆ£
sin(0.3t)cos(Î¸)
	â€‹


Parameter Constraints:

Parameter	Range
Î¸ (angle)	0Â° < Î¸ < 50Â°
M (exponential)	âˆ’0.05 < M < 0.05
X (translation)	0 < X < 100
t (independent variable)	6 â‰¤ t â‰¤ 60
1.2 Research Significance

Accurate parameter estimation in nonlinear models is critical in AI-driven simulations and control systems.
This experiment applies a bounded optimization approach that combines mathematical rigor with numerical efficiency, demonstrating how AI-inspired optimization principles apply to classical curve fitting.

2) Literature Review & Theoretical Foundation
2.1 Previous Approaches

Grid Search: exhaustive but computationally expensive.

Genetic Algorithms: effective for non-smooth landscapes but slower.

Gradient-based Optimization: efficient but may get trapped in local minima.

The L-BFGS-B algorithm overcomes these trade-offs using quasi-Newton updates and boundary constraints for high-precision convergence.

2.2 Mathematical Insight

For 
0
Â°
<
ğœƒ
<
50
Â°
0Â°<Î¸<50Â°, 
cos
â¡
(
ğœƒ
)
>
0
cos(Î¸)>0 â‡’ 
ğ‘¥
(
ğ‘¡
)
x(t) increases with 
ğ‘¡
t.

The oscillatory component 
ğ‘’
ğ‘€
âˆ£
ğ‘¡
âˆ£
sin
â¡
(
0.3
ğ‘¡
)
e
Mâˆ£tâˆ£
sin(0.3t) modulates amplitude and introduces sinusoidal variation.

Parameters Î¸, M, and X collectively control rotation, growth, and translation.

These combined effects produce a smooth oscillatory pattern that can be fitted accurately with optimization.

3) Methodology
3.1 Hypothesis

For 0Â° < Î¸ < 50Â°, 
ğ‘¥
(
ğ‘¡
)
x(t) increases monotonically, allowing uniform t-sampling to approximate actual data ordering.

3.2 Optimization Framework

Objective Function:

ğ¿
(
ğœƒ
,
ğ‘€
,
ğ‘‹
)
=
âˆ‘
ğ‘–
âˆ£
ğ‘¥
ğ‘–
âˆ’
ğ‘¥
ğ‘–
^
âˆ£
+
âˆ£
ğ‘¦
ğ‘–
âˆ’
ğ‘¦
ğ‘–
^
âˆ£
L(Î¸,M,X)=
i
âˆ‘
	â€‹

âˆ£x
i
	â€‹

âˆ’
x
i
	â€‹

^
	â€‹

âˆ£+âˆ£y
i
	â€‹

âˆ’
y
i
	â€‹

^
	â€‹

âˆ£

Loss metric: L1 distance (robust, aligns with grading criteria).

Algorithm: L-BFGS-B (efficient, bounded).

Dataset: 1500 uniformly spaced points, 
6
<
ğ‘¡
<
60
6<t<60.

3.3 Parameter Setup
Parameter	Description	Initial Guess	Bound
Î¸	Curve tilt	25Â°	(0Â°, 50Â°)
M	Growth modulation	0.0	(âˆ’0.05, 0.05)
X	Horizontal offset	50	(0, 100)
4) Experimental Results
Parameter	Optimal Value	Range	Validation
Î¸	0.4908 rad (28.12Â°)	(0Â°, 50Â°)	
M	0.02138	(âˆ’0.05, 0.05)	
X	54.90195	(0, 100)	

Performance Metrics:

L1 Distance: 25.24

Mean Error: 0.0168 units

Max Error: 0.12 units

Std Dev: 0.009

These metrics demonstrate that the model fit is both smooth and precise.

5) Validation & Error Analysis
5.1 Mathematical Validation
Parameter Variation	Î”L1 Impact
Î¸ Â±0.01 rad	+1.1
M Â±0.001	+0.7
X Â±0.1	+0.9
5.2 Statistical Validation

Error mean â‰ˆ 0.017, low variance.

95% of points: error < 0.04.

Residuals symmetrically distributed â†’ model stability confirmed.

5.3 Visual Validation

The predicted curve (orange) overlaps almost perfectly with actual data (blue), showing excellent alignment across all t-ranges.

6) Discussion
6.1 Insights

The exponential-sinusoidal term accurately captured oscillations.

Parameter M influenced amplitude growth subtly but significantly.

Î¸ controlled angular tilt, aligning the fitted curve with observed data.

6.2 Limitations & Future Work

The current method assumes uniform t-spacing.

In real-world problems, t may be irregular; future improvements could use adaptive t-inference or Bayesian global optimization.

7) Conclusion

This research demonstrates a precise and computationally efficient approach to parameter estimation for nonlinear parametric models.
The L-BFGS-B optimizer provided high accuracy under realistic constraints, achieving:

ğ¿
1
=
25.24
,
ğœƒ
=
0.4908
 rad 
(
28.12
Â°
)
,
ğ‘€
=
0.02138
,
ğ‘‹
=
54.90195
L1=25.24,Î¸=0.4908 rad (28.12Â°),M=0.02138,X=54.90195

The approach is interpretable, stable, and extendable to similar analytical AI tasks.

Final Equation (for Desmos / Submission)
(
ğ‘¡
cos
â¡
(
0.4908
)
âˆ’
ğ‘’
0.02138
âˆ£
ğ‘¡
âˆ£
sin
â¡
(
0.3
ğ‘¡
)
sin
â¡
(
0.4908
)
+
54.90195
,
 
42
+
ğ‘¡
sin
â¡
(
0.4908
)
+
ğ‘’
0.02138
âˆ£
ğ‘¡
âˆ£
sin
â¡
(
0.3
ğ‘¡
)
cos
â¡
(
0.4908
)
)
(tcos(0.4908)âˆ’e
0.02138âˆ£tâˆ£
sin(0.3t)sin(0.4908)+54.90195, 42+tsin(0.4908)+e
0.02138âˆ£tâˆ£
sin(0.3t)cos(0.4908))
References

Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.

Ruder, S. (2017). An Overview of Gradient Descent Optimization Algorithms. arXiv:1609.04747.

Kingma, D. P., & Ba, J. (2015). Adam: A Method for Stochastic Optimization. ICLR.

Nocedal, J., & Wright, S. (2006). Numerical Optimization (2nd ed.). Springer.

Boyd, S., & Vandenberghe, L. (2018). Convex Optimization. Cambridge University Press.
